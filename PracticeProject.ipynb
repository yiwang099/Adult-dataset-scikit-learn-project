{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project: Adult Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Classification with Categorical Data\n",
    "\n",
    "Preprocess the data to use only the categorical attributes and then build a Decision Tree and a Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Preprocessing Complete.\n",
      "Training data shape: (30162, 98)\n",
      "Test data shape: (15060, 98)\n"
     ]
    }
   ],
   "source": [
    "train_path = 'adult/adult.data'\n",
    "test_path = 'adult/adult.test'\n",
    "\n",
    "def preprocess_categorical_only(train_path, test_path):\n",
    "   \n",
    "    # Define column names\n",
    "    columns = [\n",
    "        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "        \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "        \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "    ]\n",
    "\n",
    "    # Read train and test data\n",
    "    df_train = pd.read_csv(train_path, header=None, names=columns, na_values=\" ?\")\n",
    "    df_test = pd.read_csv(test_path, header=None, names=columns, na_values=\" ?\", skiprows=1)\n",
    "\n",
    "    # --- Data Cleaning ---\n",
    "    # Drop rows with any missing values\n",
    "    df_train.dropna(inplace=True)\n",
    "    df_test.dropna(inplace=True)\n",
    "\n",
    "    # Clean the income column in the test set (removes trailing period)\n",
    "    df_test['income'] = df_test['income'].str.replace(r'\\.', '', regex=True)\n",
    "\n",
    "    # --- Feature Selection ---\n",
    "    # Identify continuous columns to drop\n",
    "    continuous_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "    df_train = df_train.drop(columns=continuous_cols)\n",
    "    df_test = df_test.drop(columns=continuous_cols)\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X_train_cat = df_train.drop(columns=['income'])\n",
    "    y_train = df_train['income']\n",
    "    X_test_cat = df_test.drop(columns=['income'])\n",
    "    y_test = df_test['income']\n",
    "\n",
    "    # --- Encoding ---\n",
    "    # Apply one-hot encoding\n",
    "    X_train = pd.get_dummies(X_train_cat)\n",
    "    X_test = pd.get_dummies(X_test_cat)\n",
    "    \n",
    "    # Align columns between train and test sets to ensure they have the same features\n",
    "    train_cols = X_train.columns\n",
    "    test_cols = X_test.columns\n",
    "    missing_in_test = set(train_cols) - set(test_cols)\n",
    "    for c in missing_in_test:\n",
    "        X_test[c] = 0\n",
    "    missing_in_train = set(test_cols) - set(train_cols)\n",
    "    for c in missing_in_train:\n",
    "        X_train[c] = 0\n",
    "    X_test = X_test[train_cols] # Ensure order is the same\n",
    "\n",
    "    # Encode the target variable (<=50K -> 0, >50K -> 1)\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    print(\"Part 1 Preprocessing Complete.\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Execute the function\n",
    "X_train_cat, y_train_cat, X_test_cat, y_test_cat = preprocess_categorical_only('adult/adult.data', 'adult/adult.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "\n",
    "Trains a Decision Tree classifier on the preprocessed categorical data and evaluates its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Decision Tree Classifier ---\n",
      "Accuracy: 0.8120185922974767\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.86      0.89      0.88     11360\n",
      "        >50K       0.63      0.56      0.59      3700\n",
      "\n",
      "    accuracy                           0.81     15060\n",
      "   macro avg       0.75      0.73      0.74     15060\n",
      "weighted avg       0.81      0.81      0.81     15060\n",
      "\n",
      "True Positive Rate (Recall) for '>50K': 0.5597\n",
      "False Positive Rate for '>50K': 0.1058\n"
     ]
    }
   ],
   "source": [
    "def run_decision_tree(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print(\"\\n--- Running Decision Tree Classifier ---\")\n",
    "    # Initialize and train the model\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "    dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['<=50K', '>50K']))\n",
    "\n",
    "    # Calculate and print TP and FP rates from the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tp_rate = tp / (tp + fn)\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    print(f\"True Positive Rate (Recall) for '>50K': {tp_rate:.4f}\")\n",
    "    print(f\"False Positive Rate for '>50K': {fp_rate:.4f}\")\n",
    "\n",
    "# Execute the Decision Tree function\n",
    "run_decision_tree(X_train_cat, y_train_cat, X_test_cat, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Classifier\n",
    "\n",
    "Trains a Naïve Bayes classifier. Given that our data is now a sparse, count-based matrix from one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Naïve Bayes Classifier ---\n",
      "Accuracy: 0.7940239043824702\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.91      0.81      0.86     11360\n",
      "        >50K       0.56      0.74      0.64      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.73      0.78      0.75     15060\n",
      "weighted avg       0.82      0.79      0.80     15060\n",
      "\n",
      "True Positive Rate (Recall) for '>50K': 0.7400\n",
      "False Positive Rate for '>50K': 0.1884\n"
     ]
    }
   ],
   "source": [
    "def run_naive_bayes(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print(\"\\n--- Running Naïve Bayes Classifier ---\")\n",
    "    # Initialize and train the model\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['<=50K', '>50K']))\n",
    "    \n",
    "    # Calculate and print TP and FP rates\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tp_rate = tp / (tp + fn)\n",
    "    fp_rate = fp / (fp + tn)\n",
    "    print(f\"True Positive Rate (Recall) for '>50K': {tp_rate:.4f}\")\n",
    "    print(f\"False Positive Rate for '>50K': {fp_rate:.4f}\")\n",
    "\n",
    "# Execute the Naive Bayes function\n",
    "run_naive_bayes(X_train_cat, y_train_cat, X_test_cat, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Clustering & Classification with Transformed Data\n",
    "\n",
    "Numerical data will be binarized based on its mean, and categorical data will be one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n",
    "\n",
    "Clean unknown values, binarizing numerical attributes values above the column's mean become `1`, and others become `0`. and One-hot encoding categorical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (30162, 104)\n",
      "Test data shape: (15060, 104)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_mixed_transformed(train_path, test_path):\n",
    "\n",
    "    # Define column names\n",
    "    columns = [\n",
    "        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "        \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "        \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "    ]\n",
    "    \n",
    "    df_train = pd.read_csv(train_path, header=None, names=columns, na_values=\" ?\")\n",
    "    df_test = pd.read_csv(test_path, header=None, names=columns, na_values=\" ?\", skiprows=1)\n",
    "\n",
    "    df_train.dropna(inplace=True)\n",
    "    df_test.dropna(inplace=True)\n",
    "    df_test['income'] = df_test['income'].str.replace(r'\\.', '', regex=True)\n",
    "\n",
    "    # Combine for consistent processing\n",
    "    combined_df = pd.concat([df_train, df_test], axis=0)\n",
    "\n",
    "    # --- Feature Transformation ---\n",
    "    numerical_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "    categorical_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "\n",
    "    # Binarize numerical columns\n",
    "    for col in numerical_cols:\n",
    "        mean_val = combined_df[col].mean()\n",
    "        combined_df[col] = (combined_df[col] > mean_val).astype(int)\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    combined_df = pd.get_dummies(combined_df, columns=categorical_cols)\n",
    "\n",
    "    # Encode target variable\n",
    "    combined_df['income'] = LabelEncoder().fit_transform(combined_df['income'])\n",
    "\n",
    "    # Split back into train and test sets\n",
    "    train_size = len(df_train)\n",
    "    df_train_processed = combined_df.iloc[:train_size]\n",
    "    df_test_processed = combined_df.iloc[train_size:]\n",
    "\n",
    "    X_train = df_train_processed.drop(columns=['income'])\n",
    "    y_train = df_train_processed['income']\n",
    "    X_test = df_test_processed.drop(columns=['income'])\n",
    "    y_test = df_test_processed['income']\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Execute the function\n",
    "X_train_mix, y_train_mix, X_test_mix, y_test_mix = preprocess_mixed_transformed('adult/adult.data', 'adult/adult.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means Clustering\n",
    "\n",
    "Applie the k-Means algorithm to the transformed training data for `k` values of 3, 5, and 10, reporting the centroids for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running k-Means Clustering ---\n",
      "\n",
      "Running with k = 3\n",
      "Centroids for 3 clusters (showing first 5 dimensions for brevity):\n",
      "[[0.64366135 0.40619974 0.30443047 0.06755374 0.03772481]\n",
      " [0.60964564 0.42587001 0.37486096 0.12148419 0.06523121]\n",
      " [0.17490919 0.47350284 0.28900065 0.04461209 0.03241129]]\n",
      "\n",
      "Running with k = 5\n",
      "Centroids for 5 clusters (showing first 5 dimensions for brevity):\n",
      "[[ 4.18361418e-01  4.30563626e-01  2.22254503e-02  3.74782103e-02\n",
      "   2.62928530e-02]\n",
      " [ 6.44661431e-01  4.16472087e-01  1.00000000e+00  1.69178518e-01\n",
      "   8.83039694e-02]\n",
      " [ 2.40459304e-01  4.85309017e-01  4.06957109e-02  3.93448159e-02\n",
      "   2.97196893e-02]\n",
      " [ 5.88962529e-01  4.31302575e-01 -2.38697950e-15  9.25312261e-02\n",
      "   5.13637522e-02]\n",
      " [ 4.14878100e-01  4.24671807e-01  9.99791623e-01  9.50197958e-02\n",
      "   5.23025630e-02]]\n",
      "\n",
      "Running with k = 10\n",
      "Centroids for 10 clusters (showing first 5 dimensions for brevity):\n",
      "[[6.63719422e-01 3.94046566e-01 1.85676393e-02 5.30503979e-02\n",
      "  3.15355143e-02]\n",
      " [7.23470662e-01 3.68913858e-01 3.32084894e-01 1.19850187e-01\n",
      "  6.36704120e-02]\n",
      " [5.85196375e-01 4.61329305e-01 3.88578059e-16 9.63746224e-02\n",
      "  5.19637462e-02]\n",
      " [6.34860652e-01 4.22531885e-01 1.00000000e+00 1.71469060e-01\n",
      "  8.92772792e-02]\n",
      " [4.61852861e-01 5.36103542e-01 1.62125341e-01 3.81471390e-02\n",
      "  2.17983651e-02]\n",
      " [8.29326923e-02 4.26682692e-01 3.56570513e-02 2.08333333e-02\n",
      "  2.12339744e-02]\n",
      " [4.30740741e-01 3.92592593e-01 9.99629630e-01 9.44444444e-02\n",
      "  5.66666667e-02]\n",
      " [3.79326152e-01 4.81778593e-01 3.69470548e-01 7.01352281e-02\n",
      "  4.40064176e-02]\n",
      " [5.59584036e-01 4.18493536e-01 3.88578059e-16 8.76897133e-02\n",
      "  4.86228218e-02]\n",
      " [7.34035550e-02 4.85516787e-01 1.43515471e-01 2.36998025e-02\n",
      "  2.13956550e-02]]\n"
     ]
    }
   ],
   "source": [
    "def run_kmeans(X_train, k_values=[3, 5, 10]):\n",
    "\n",
    "    print(\"\\n--- Running k-Means Clustering ---\")\n",
    "    for k in k_values:\n",
    "        print(f\"\\nRunning with k = {k}\")\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        kmeans.fit(X_train)\n",
    "        print(f\"Centroids for {k} clusters (showing first 5 dimensions for brevity):\")\n",
    "        # Centroids can be very large, so we print a snippet\n",
    "        print(kmeans.cluster_centers_[:, :5])\n",
    "\n",
    "# Execute the k-Means function\n",
    "run_kmeans(X_train_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors (kNN)\n",
    "\n",
    "Train kNN classifier and uses it to predict the class for the last 10 records of the test data, testing `k` values of 3, 5, and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running k-Nearest Neighbors (kNN) ---\n",
      "Prediction accuracy for k = 3 on the last 10 records: 0.70\n",
      "Prediction accuracy for k = 5 on the last 10 records: 0.80\n",
      "Prediction accuracy for k = 10 on the last 10 records: 0.80\n"
     ]
    }
   ],
   "source": [
    "def run_knn(X_train, y_train, X_test, y_test, k_values=[3, 5, 10]):\n",
    "\n",
    "    print(\"\\n--- Running k-Nearest Neighbors (kNN) ---\")\n",
    "    # Use the last 10 records for prediction\n",
    "    X_test_subset = X_test.tail(10)\n",
    "    y_test_subset = y_test.tail(10)\n",
    "    \n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test_subset)\n",
    "        accuracy = accuracy_score(y_test_subset, y_pred)\n",
    "        print(f\"Prediction accuracy for k = {k} on the last 10 records: {accuracy:.2f}\")\n",
    "\n",
    "# Execute the kNN function\n",
    "run_knn(X_train_mix, y_train_mix, X_test_mix, y_test_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "Build SVM classifier on the transformed data and reports its accuracy on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Support Vector Machine (SVM) ---\n",
      "SVM prediction accuracy on the test data: 0.8435\n"
     ]
    }
   ],
   "source": [
    "def run_svm(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print(\"\\n--- Running Support Vector Machine (SVM) ---\")\n",
    "    svm_classifier = SVC(kernel='linear', random_state=42) # Using a linear kernel for efficiency\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"SVM prediction accuracy on the test data: {accuracy:.4f}\")\n",
    "\n",
    "# Execute the SVM function\n",
    "run_svm(X_train_mix, y_train_mix, X_test_mix, y_test_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "Build Neural Network (Multi-layer Perceptron) classifier and reports its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Neural Network ---\n",
      "Neural Network prediction accuracy on the test data: 0.8116\n"
     ]
    }
   ],
   "source": [
    "def run_neural_network(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print(\"\\n--- Running Neural Network ---\")\n",
    "    # A simple NN with two hidden layers\n",
    "    nn_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "    nn_classifier.fit(X_train, y_train)\n",
    "    y_pred = nn_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Neural Network prediction accuracy on the test data: {accuracy:.4f}\")\n",
    "\n",
    "# Execute the Neural Network function\n",
    "run_neural_network(X_train_mix, y_train_mix, X_test_mix, y_test_mix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
